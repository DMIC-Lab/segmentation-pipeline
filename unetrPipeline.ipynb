{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cc3d\n",
    "import torch\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import subprocess\n",
    "import random\n",
    "import scipy.ndimage as ndimage\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from monai.losses import DiceLoss\n",
    "import nibabel as nib\n",
    "import napari\n",
    "from skimage import morphology\n",
    "from cc_torch import connected_components_labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseModelPath = \"/home/gabriela/Documents/gabiSegmentationProject/newCOPD_coarse_model/weights/19_v2_half.pth\"\n",
    "LRModelPath = \"/home/gabriela/Documents/gabiSegmentationProject/newCOPD_LR_model/weights/14_v3_half.pth\"\n",
    "rightModelPath = \"/home/gabriela/Documents/gabiSegmentationProject/newCOPD_right_model/weights/41_v2_half.pth\"\n",
    "leftModelPath = \"/home/gabriela/Documents/gabiSegmentationProject/newCOPD_left_model/weights/17_v3_half.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oneFolderFinalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imageDirectory):\n",
    "        #self.imageList = sorted([file for file in os.listdir(imageDirectory) if \"image\" in file])\n",
    "        self.imageList = sorted([file for file in os.listdir(imageDirectory)])\n",
    "        #self.labelList = sorted([file for file in os.listdir(imageDirectory) if \"mask\" in file])\n",
    "        self.imageDirectory = imageDirectory\n",
    "    \n",
    "    def __len__(self):\n",
    "        x = len(self.imageList)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imagePath = os.path.join(self.imageDirectory, self.imageList[idx])\n",
    "        #labelPath = os.path.join(self.imageDirectory, self.labelList[idx])\n",
    "        #image = nib.load(imagePath).get_fdata()\n",
    "        matfile = loadmat(imagePath)\n",
    "        inhale = np.array(matfile['T00'], dtype=np.int32)\n",
    "        exhale = np.array(matfile['T50'], dtype=np.int32)\n",
    "        print(np.min(inhale),np.max(inhale))\n",
    "        inhale = 1 + ((inhale - 1024) / 1000)\n",
    "        inhale = np.clip(inhale, 0, 1) * 255\n",
    "        #label = nib.load(labelPath).get_fdata()\n",
    "        inhale = torch.from_numpy(inhale).float().unsqueeze(0)\n",
    "        exhale = 1 + ((exhale-1024) / 1000)\n",
    "        exhale = np.clip(exhale, 0, 1) * 255\n",
    "        #label = nib.load(labelPath).get_fdata()\n",
    "        exhale = torch.from_numpy(exhale).float().unsqueeze(0)\n",
    "        #label = torch.from_numpy(label).float().unsqueeze(0)\n",
    "\n",
    "        return inhale, exhale, self.imageList[idx]\n",
    "    \n",
    "    def findIndex(self,filename):\n",
    "        for i in range(len(self.imageList)):\n",
    "            if filename == self.imageList[i]:\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "def display_nvidia_smi_memory_usage():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total,memory.used,memory.free', '--format=csv,nounits,noheader'], check=True, text=True, stdout=subprocess.PIPE)\n",
    "        output = result.stdout.strip().split('\\n')\n",
    "        for i, line in enumerate(output):\n",
    "            total, used, free = map(int, line.split(','))\n",
    "            print(f\"GPU {i}: Total Memory: {total} MiB, Used Memory: {used} MiB, Free Memory: {free} MiB\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Failed to execute nvidia-smi. Make sure you have the utility installed and that you have NVIDIA GPUs.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "coarseModel = SwinUNETR(img_size=(128,128,128), in_channels=1, out_channels=2, feature_size=12)\n",
    "coarseModel.load_state_dict(torch.load(coarseModelPath))\n",
    "coarseModel.eval()\n",
    "LRModel = SwinUNETR(img_size=(128,128,128), in_channels=1, out_channels=3, feature_size=12)\n",
    "LRModel.load_state_dict(torch.load(LRModelPath))\n",
    "LRModel.eval()\n",
    "rightModel = SwinUNETR(img_size=(128,128,128), in_channels=1, out_channels=4, feature_size=12)\n",
    "rightModel.load_state_dict(torch.load(rightModelPath))\n",
    "rightModel.eval()\n",
    "leftModel = SwinUNETR(img_size=(128,128,128), in_channels=1, out_channels=3, feature_size=12)\n",
    "leftModel.load_state_dict(torch.load(leftModelPath))\n",
    "leftModel.eval()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved to cuda\n"
     ]
    }
   ],
   "source": [
    "coarseModel.to(\"cuda:0\")\n",
    "LRModel.to(\"cuda:0\")\n",
    "rightModel.to(\"cuda:0\")\n",
    "leftModel.to(\"cuda:0\")\n",
    "print('moved to cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Total Memory: 49140 MiB, Used Memory: 2078 MiB, Free Memory: 46598 MiB\n",
      "GPU 1: Total Memory: 49140 MiB, Used Memory: 13 MiB, Free Memory: 48672 MiB\n"
     ]
    }
   ],
   "source": [
    "display_nvidia_smi_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "dataset = oneFolderFinalDataset(\"/home/gabriela/Documents/validation_cases/original_cancer_cases\")\n",
    "total_samples = len(dataset)\n",
    "#train_size = int(0.8 * total_samples)\n",
    "#val_size = total_samples - train_size\n",
    "#train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator = torch.Generator().manual_seed(42))\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox2_3D(img,margin=5):\n",
    "    r = np.any(img, axis=(1, 2))\n",
    "    c = np.any(img, axis=(0, 2))\n",
    "    z = np.any(img, axis=(0, 1))\n",
    "    xmin, xmax = np.where(r)[0][[0, -1]]\n",
    "    ymin, ymax = np.where(c)[0][[0, -1]]\n",
    "    zmin, zmax = np.where(z)[0][[0, -1]]\n",
    "    xmin = max(0,xmin-margin)\n",
    "    xmax = min(img.shape[0],xmax+margin)\n",
    "    ymin = max(0,ymin-margin)\n",
    "    ymax = min(img.shape[1],ymax+margin)\n",
    "    zmin = max(0,zmin-margin)\n",
    "    zmax = min(img.shape[2],zmax+margin)\n",
    "    return xmin, xmax, ymin, ymax, zmin, zmax\n",
    "\n",
    "def getLargest(arr, num=None):\n",
    "    if num is None:\n",
    "        num = len(np.unique(arr))-1\n",
    "    # Use cc3d to find connected components\n",
    "    connected_components = cc3d.connected_components(arr)\n",
    "\n",
    "    # Count volume for each component\n",
    "    volume_count = {}\n",
    "    for component in np.unique(connected_components):\n",
    "        if component == 0:  # background\n",
    "            continue\n",
    "        volume_count[component] = np.sum(connected_components == component)\n",
    "\n",
    "    # Sort by volume\n",
    "    sorted_components = sorted(volume_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Take two largest components\n",
    "    largest_components = [comp_id for comp_id, _ in sorted_components[:num]]\n",
    "\n",
    "    # Create a new 3D array with only the two largest components\n",
    "    new_array_3d = np.zeros_like(arr)\n",
    "    for comp_id in largest_components:\n",
    "        new_array_3d[connected_components == comp_id] = np.unique(arr[connected_components == comp_id])[0]   # or comp_id to retain original labels\n",
    "\n",
    "    return new_array_3d\n",
    "\n",
    "\n",
    "def rescaleBounds(bounds,originalImageShape,imageShape):\n",
    "    bounds = list(bounds)\n",
    "    xScale = originalImageShape[0]/imageShape[0]\n",
    "    yScale = originalImageShape[1]/imageShape[1]\n",
    "    zScale = originalImageShape[2]/imageShape[2]\n",
    "    bounds[0] = int(bounds[0]*xScale)\n",
    "    bounds[1] = int(bounds[1]*xScale)\n",
    "    bounds[2] = int(bounds[2]*yScale)\n",
    "    bounds[3] = int(bounds[3]*yScale)\n",
    "    bounds[4] = int(bounds[4]*zScale)\n",
    "    bounds[5] = int(bounds[5]*zScale)\n",
    "    return bounds\n",
    "\n",
    "def fitInBounds(rescaledBounds, intermediateImageBounds):\n",
    "    finalBounds = list(rescaledBounds)\n",
    "    \n",
    "    # The intermediateImageBounds should have format [x_min, x_max, y_min, y_max, z_min, z_max]\n",
    "    \n",
    "    # adjust the x coordinates\n",
    "    finalBounds[0] = rescaledBounds[0] + intermediateImageBounds[0]\n",
    "    finalBounds[1] = rescaledBounds[1] + intermediateImageBounds[0]\n",
    "    \n",
    "    # adjust the y coordinates\n",
    "    finalBounds[2] = rescaledBounds[2] + intermediateImageBounds[2]\n",
    "    finalBounds[3] = rescaledBounds[3] + intermediateImageBounds[2]\n",
    "    \n",
    "    # adjust the z coordinates\n",
    "    finalBounds[4] = rescaledBounds[4] + intermediateImageBounds[4]\n",
    "    finalBounds[5] = rescaledBounds[5] + intermediateImageBounds[4]\n",
    "    \n",
    "    return finalBounds\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(array1, array2, num_classes):\n",
    "    iou_list = []\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        # Create binary maps for the current class\n",
    "        binary_array1 = (array1 == cls)\n",
    "        binary_array2 = (array2 == cls)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = np.logical_and(binary_array1, binary_array2).sum()\n",
    "        union = np.logical_or(binary_array1, binary_array2).sum()\n",
    "        \n",
    "        # Calculate IoU\n",
    "        if union == 0:\n",
    "            iou = 1.0  # If both arrays have 0 instances for this class, consider it a perfect match.\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "            \n",
    "        iou_list.append(iou)\n",
    "        \n",
    "    return iou_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelOutput(input,model):\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "        output = softmax(output)\n",
    "        output = torch.argmax(output,dim=1)\n",
    "        output = output.squeeze(0).cpu().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "def connectedComponents(arr, threshold=3000, connectivity = 26,num_override=0):\n",
    "    arr = cc3d.dust(arr,threshold=threshold,connectivity=connectivity,in_place=False)\n",
    "    if num_override:\n",
    "        arr = getLargest(arr,num_override)\n",
    "    else:\n",
    "        arr = getLargest(arr)\n",
    "    return arr\n",
    "\n",
    "def crop(originalImage, bounds):\n",
    "    return originalImage[bounds[0]:bounds[1],bounds[2]:bounds[3],bounds[4]:bounds[5]]\n",
    "\n",
    "def prepInput(image,device):\n",
    "    image = ndimage.zoom(image, (128/image.shape[0], 128/image.shape[1], 128/image.shape[2]), order=0)\n",
    "    image = torch.from_numpy(image).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "def bodyMorphology(image):\n",
    "    smallImage = ndimage.zoom(image, (64/image.shape[0], 64/image.shape[1], 64/image.shape[2]), order=0)\n",
    "    imageThresholded = np.where(smallImage > 128, 1, 0)\n",
    "    distances = ndimage.distance_transform_edt(imageThresholded)\n",
    "    shrunkMask = distances > 1\n",
    "    workingImage = shrunkMask\n",
    "    for i in range(3):\n",
    "        workingImage = ndimage.binary_dilation(workingImage,structure=morphology.ball(1),iterations=2)\n",
    "\n",
    "        workingImage = ndimage.binary_erosion(workingImage,structure=morphology.ball(1),iterations=1)\n",
    "\n",
    "        workingImage = ndimage.binary_closing(workingImage,structure=morphology.ball(5),iterations=1)\n",
    "        workingImage = np.where(shrunkMask,1,workingImage)\n",
    "\n",
    "    workingImage = ndimage.binary_erosion(workingImage,structure=morphology.ball(1),iterations=1)\n",
    "    workingImage = ndimage.zoom(workingImage, (image.shape[0]/64, image.shape[1]/64, image.shape[2]/64), order=0)\n",
    "\n",
    "    return workingImage\n",
    "\n",
    "def newMorphology(image):\n",
    "    smallImage = ndimage.zoom(image,(128/image.shape[0],128/image.shape[1],128/image.shape[2]),order=0)\n",
    "    thresholdedImage = smallImage > 128\n",
    "    #eroded = ndimage.binary_erosion(dustedImage,structure=morphology.ball(1))\n",
    "    flipped = np.logical_not(thresholdedImage)\n",
    "    background = connectedComponents(flipped)\n",
    "    final = np.logical_not(background)\n",
    "    final = ndimage.binary_erosion(final,structure=morphology.ball(2))\n",
    "    return final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorchGetLargest(tensor, num = None, threshold=3000, ignoreBackground=True,override_comp_id=None):\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = tensor.squeeze(0)\n",
    "    if len(tensor.shape) == 5:\n",
    "        tensor = tensor.squeeze(0).squeeze(0)\n",
    "    if tensor.dtype != torch.uint8:\n",
    "        tensor = tensor.to(torch.uint8)\n",
    "    test = (tensor > 0).to(torch.uint8)\n",
    "    connected_components = connected_components_labeling(test)\n",
    "    if override_comp_id is None:\n",
    "        volume_count = {}\n",
    "        for component in connected_components.unique():\n",
    "            if component == 0 and ignoreBackground:  # background\n",
    "                continue\n",
    "            count = torch.sum(connected_components == component).item()\n",
    "            if count < threshold:\n",
    "                continue\n",
    "            volume_count[component.item()] = count\n",
    "        if num is None:\n",
    "            num = len(volume_count)\n",
    "        # Sort by volume\n",
    "        sorted_components = sorted(volume_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Take two largest components\n",
    "        largest_components = [comp_id for comp_id, _ in sorted_components[:num]]\n",
    "    else:\n",
    "        largest_components = override_comp_id\n",
    "\n",
    "    # Create a new 3D array with only the two largest components\n",
    "    new_array_3d = torch.zeros_like(tensor)\n",
    "    for comp_id in largest_components:\n",
    "        new_array_3d[connected_components == comp_id] = tensor[connected_components == comp_id]   # or comp_id to retain original labels\n",
    "\n",
    "    return new_array_3d.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "def torchGetModelOutput(input,model):\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "        output = softmax(output)\n",
    "        output = torch.argmax(output,dim=1).unsqueeze(0)\n",
    "    return output.to(torch.uint8)\n",
    "\n",
    "def pytorchBinaryErosion(tensor):\n",
    "    ball = morphology.ball(2)\n",
    "    struct_elem = torch.tensor(ball, dtype=torch.float32)\n",
    "    struct_elem = struct_elem.view(1, 1, *struct_elem.size()).cuda()\n",
    "\n",
    "    # Perform 3D convolution with structuring element\n",
    "    conv_result = torch.nn.functional.conv3d(tensor.float(), struct_elem, padding=2)\n",
    "\n",
    "    # Binary erosion is equivalent to finding where the convolution result\n",
    "    # is equal to the sum of the structuring element\n",
    "    erosion_result = (conv_result == struct_elem.sum().item()).float()\n",
    "    return erosion_result\n",
    "\n",
    "def torchMorphology(smallTensor):\n",
    "    thresholdedTensor = (smallTensor > 128)\n",
    "    flippedTensor = torch.logical_not(thresholdedTensor).to(torch.uint8)\n",
    "    ccTensor = pytorchGetLargest(flippedTensor,ignoreBackground=False,override_comp_id=[flippedTensor[:,:,0,0,0].item()])\n",
    "    finalTensor = pytorchBinaryErosion(torch.logical_not(ccTensor))\n",
    "    return finalTensor\n",
    "\n",
    "def torchbbox2_3D(img,margin=5):\n",
    "    if len(img.shape) > 3:\n",
    "        for i in range(len(img.shape)-3):\n",
    "            img = img.squeeze(0)\n",
    "    r = img.any(dim=2).any(dim=1)\n",
    "    c = img.any(dim=2).any(dim=0)\n",
    "    z = img.any(dim=1).any(dim=0)\n",
    "    xmin, xmax = torch.where(r)[0][[0, -1]]\n",
    "    ymin, ymax = torch.where(c)[0][[0, -1]]\n",
    "    zmin, zmax = torch.where(z)[0][[0, -1]]\n",
    "    \n",
    "    xmin = max(0,xmin-margin)\n",
    "    xmax = min(img.shape[0],xmax+margin)\n",
    "    ymin = max(0,ymin-margin)\n",
    "    ymax = min(img.shape[1],ymax+margin)\n",
    "    zmin = max(0,zmin-margin)\n",
    "    zmax = min(img.shape[2],zmax+margin)\n",
    "    return int(xmin), int(xmax), int(ymin), int(ymax), int(zmin), int(zmax)\n",
    "\n",
    "def torchPrep(image):\n",
    "    print(image.shape)\n",
    "    image = torch.nn.functional.interpolate(image,size=(128,128,128),mode='nearest')\n",
    "    return image\n",
    "\n",
    "def pytorchBinaryDilation(tensor, selem_radius=3):\n",
    "    ball = morphology.ball(selem_radius)\n",
    "    struct_elem = torch.tensor(ball, dtype=torch.float32)\n",
    "    struct_elem = struct_elem.view(1, 1, *struct_elem.size()).cuda()\n",
    "\n",
    "    # Perform 3D convolution with structuring element\n",
    "    conv_result = torch.nn.functional.conv3d(tensor.float(), struct_elem, padding=selem_radius)\n",
    "\n",
    "    # Binary dilation is equivalent to finding where the convolution result\n",
    "    # is greater than 0\n",
    "    dilation_result = (conv_result > 0).float()\n",
    "\n",
    "    return dilation_result\n",
    "\n",
    "def torchCrop(originalImage, bounds):\n",
    "    return originalImage[:,:,bounds[0]:bounds[1],bounds[2]:bounds[3],bounds[4]:bounds[5]]\n",
    "\n",
    "def torchRescaleBounds(bounds,originalImageShape,imageShape):\n",
    "    bounds = list(bounds)\n",
    "    xScale = originalImageShape[2]/imageShape[2]\n",
    "    yScale = originalImageShape[3]/imageShape[3]\n",
    "    zScale = originalImageShape[4]/imageShape[4]\n",
    "    bounds[0] = int(bounds[0]*xScale)\n",
    "    bounds[1] = int(bounds[1]*xScale)\n",
    "    bounds[2] = int(bounds[2]*yScale)\n",
    "    bounds[3] = int(bounds[3]*yScale)\n",
    "    bounds[4] = int(bounds[4]*zScale)\n",
    "    bounds[5] = int(bounds[5]*zScale)\n",
    "    return bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullPipeline(originalImage,coarseModel,LRModel,leftModel,rightModel,device, diagnostics= [False,False,False,False,False,False]):\n",
    "    selem = morphology.ball(3)\n",
    "    originalImage = originalImage.to(device)\n",
    "    coarseImage = torch.nn.functional.interpolate(originalImage,size=(128,128,128),mode='nearest')\n",
    "    originalImage = originalImage.squeeze(0).squeeze(0).cpu().numpy()\n",
    "    if diagnostics[0]:\n",
    "       viewer1 = napari.view_image(originalImage)\n",
    "    \n",
    "    #Segment lung - coarse model outputs binary mask of what is lung and not\n",
    "    coarseOutput = getModelOutput(coarseImage,coarseModel)\n",
    "    coarseImage = coarseImage.squeeze(0).squeeze(0).cpu().numpy()\n",
    "    bodyMask = newMorphology(coarseImage)\n",
    "    coarseOutput = np.where(bodyMask,coarseOutput,0)\n",
    "    coarseOutput = connectedComponents(coarseOutput,num_override=2)\n",
    "    coarseBounds128 = bbox2_3D(coarseOutput)\n",
    "    coarseBounds = rescaleBounds(coarseBounds128,originalImage.shape,coarseOutput.shape)\n",
    "    coarseCropped = crop(originalImage,coarseBounds)\n",
    "    if diagnostics[1]:\n",
    "        coarseModelViewer= napari.view_image(coarseImage)\n",
    "        coarseModelViewer.add_image(coarseOutput)\n",
    "        viewer2 = napari.view_image(originalImage)\n",
    "        coarseBox = np.zeros(originalImage.shape)\n",
    "        coarseBox[coarseBounds[0]:coarseBounds[1],coarseBounds[2]:coarseBounds[3],coarseBounds[4]:coarseBounds[5]] = 1\n",
    "        viewer2.add_image(coarseBox)\n",
    "    \n",
    "    #Segment LR - LR model outputs mask of what is left and right lung\n",
    "    LRInput = prepInput(coarseCropped,device)\n",
    "    LROutput = getModelOutput(LRInput,LRModel)\n",
    "    LROutput = connectedComponents(LROutput)\n",
    "    leftOutput = np.where(LROutput==1,1,0)\n",
    "    rightOutput = np.where(LROutput==2,1,0)\n",
    "    leftBounds128 = bbox2_3D(leftOutput,margin=1)\n",
    "    rightBounds128 = bbox2_3D(rightOutput,margin=1)\n",
    "    leftCoarseBounds = rescaleBounds(leftBounds128,coarseCropped.shape,leftOutput.shape)\n",
    "    rightCoarseBounds = rescaleBounds(rightBounds128,coarseCropped.shape,rightOutput.shape)\n",
    "    leftBounds = fitInBounds(leftCoarseBounds,coarseBounds)\n",
    "    rightBounds = fitInBounds(rightCoarseBounds,coarseBounds)\n",
    "    leftCropped = crop(originalImage,leftBounds)\n",
    "    rightCropped = crop(originalImage,rightBounds)\n",
    "\n",
    "    LRFullSizeMask = np.zeros(originalImage.shape)\n",
    "    LRCoarseSize = ndimage.zoom(LROutput, (coarseCropped.shape[0]/128,coarseCropped.shape[1]/128,coarseCropped.shape[2]/128), order=0)\n",
    "    LRFullSizeMask[coarseBounds[0]:coarseBounds[1],coarseBounds[2]:coarseBounds[3],coarseBounds[4]:coarseBounds[5]] = LRCoarseSize\n",
    "    LRFullSizeMask = np.where(LRFullSizeMask,1,0)\n",
    "    LRMaskDilated = morphology.binary_dilation(LRFullSizeMask,footprint=selem)\n",
    "\n",
    "    if diagnostics[2]:\n",
    "       viewer3 = napari.view_image(originalImage)\n",
    "       viewer3.add_image(LRFullSizeMask,colormap=\"gist_earth\",contrast_limits=(0,2),opacity=.5)\n",
    "    \n",
    "    # Get and post-process left lobe model output\n",
    "    leftInput = prepInput(leftCropped,device)\n",
    "    leftLobeOutput = getModelOutput(leftInput,leftModel)\n",
    "    if diagnostics[3]:\n",
    "        leftLobeViewer = napari.view_image(leftInput.squeeze(0).squeeze(0).cpu().numpy())\n",
    "        leftLobeViewer.add_image(leftLobeOutput)\n",
    "    leftLobeOutput = ndimage.zoom(leftLobeOutput, (leftCropped.shape[0]/128,leftCropped.shape[1]/128,leftCropped.shape[2]/128), order=0)\n",
    "    \n",
    "    # Get and post-process right lobe model output\n",
    "    rightInput = prepInput(rightCropped,device)\n",
    "    rightLobeOutput = getModelOutput(rightInput,rightModel)       \n",
    "    if diagnostics[4]:\n",
    "        rightLobeViewer = napari.view_image(rightInput.squeeze(0).squeeze(0).cpu().numpy())\n",
    "        rightLobeViewer.add_image(rightLobeOutput)\n",
    "    rightLobeOutput = ndimage.zoom(rightLobeOutput, (rightCropped.shape[0]/128,rightCropped.shape[1]/128,rightCropped.shape[2]/128), order=0)\n",
    "\n",
    "\n",
    "    #adjust right lobe output (0,1,2,3) to (0,3,4,5)\n",
    "    rightLobeOutput = rightLobeOutput + 2\n",
    "    rightLobeOutput = np.where(rightLobeOutput==2,0,rightLobeOutput)\n",
    "    \n",
    "    #Assemble final mask\n",
    "    finalMask = np.zeros(originalImage.shape)\n",
    "    leftFullSize = np.zeros(originalImage.shape)\n",
    "    rightFullSize = np.zeros(originalImage.shape)\n",
    "    leftFullSize[leftBounds[0]:leftBounds[1],leftBounds[2]:leftBounds[3],leftBounds[4]:leftBounds[5]] = leftLobeOutput\n",
    "    rightFullSize[rightBounds[0]:rightBounds[1],rightBounds[2]:rightBounds[3],rightBounds[4]:rightBounds[5]] = rightLobeOutput\n",
    "    finalMask = np.where(leftFullSize, leftFullSize, finalMask)\n",
    "    finalMask = np.where(rightFullSize, rightFullSize, finalMask)\n",
    "\n",
    "    finalMask = np.where(LRMaskDilated,finalMask,0)\n",
    "    #finalMask = connectedComponents(finalMask)\n",
    "\n",
    "    finalMask = finalMask.astype(np.uint8)\n",
    "    if diagnostics[5]:\n",
    "        leftbox = np.zeros(originalImage.shape)\n",
    "        leftbox[leftBounds[0]:leftBounds[1],leftBounds[2]:leftBounds[3],leftBounds[4]:leftBounds[5]] = 1\n",
    "        rightbox = np.zeros(originalImage.shape)\n",
    "        rightbox[rightBounds[0]:rightBounds[1],rightBounds[2]:rightBounds[3],rightBounds[4]:rightBounds[5]] = 1\n",
    "        viewer4 = napari.view_image(originalImage)\n",
    "        viewer4.add_image(finalMask, colormap=\"gist_earth\",contrast_limits=(0,5),opacity=.5)\n",
    "        viewer4.add_image(leftbox,colormap=\"red\",opacity=.5)\n",
    "        viewer4.add_image(rightbox,colormap=\"blue\",opacity=.5)\n",
    "    return finalMask\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullPytorchPipeline(originalImage,coarseModel,LRModel,leftModel,rightModel,device):\n",
    "    originalImage = originalImage.to(device)\n",
    "    coarseImage = torch.nn.functional.interpolate(originalImage,size=(128,128,128),mode='nearest')    \n",
    "    #Segment lung - coarse model outputs binary mask of what is lung and not\n",
    "    coarseOutput = torchGetModelOutput(coarseImage,coarseModel)\n",
    "    \n",
    "    bodyMask = torchMorphology(coarseImage)\n",
    "    coarseOutput = torch.where(bodyMask > 0,coarseOutput,0)\n",
    "    coarseOutput = pytorchGetLargest(coarseOutput,num=2) #HWD\n",
    "    \n",
    "    coarseBounds128 = torchbbox2_3D(coarseOutput)\n",
    "    coarseBounds = torchRescaleBounds(coarseBounds128,originalImage.shape,coarseOutput.shape)\n",
    "    coarseCropped = torchCrop(originalImage,coarseBounds)\n",
    "    \n",
    "    #Segment LR - LR model outputs mask of what is left and right lung\n",
    "    LRInput = torchPrep(coarseCropped) #HWD -> NCHWD\n",
    "    LROutput = torchGetModelOutput(LRInput,LRModel)\n",
    "    print('lr')\n",
    "    LROutput = pytorchGetLargest(LROutput,num=2)\n",
    "    LRFullSizeMask = torch.zeros(originalImage.shape).cuda()\n",
    "    LRCoarseSize = torch.nn.functional.interpolate(LROutput,size=coarseCropped.shape[2:],mode='nearest-exact')\n",
    "    LRFullSizeMask[:,:,coarseBounds[0]:coarseBounds[1],coarseBounds[2]:coarseBounds[3],coarseBounds[4]:coarseBounds[5]] = LRCoarseSize\n",
    "    #iewer = napari.view_image(LROutput.cpu().numpy(),contrast_limits=(0,2))\n",
    "    \n",
    "    leftOutput = torch.where(LROutput==1,1,0)\n",
    "    rightOutput = torch.where(LROutput==2,1,0)\n",
    "    leftOutput = pytorchGetLargest(leftOutput,num=1)\n",
    "    rightOutput = pytorchGetLargest(rightOutput,num=1)\n",
    "    leftBounds128 = torchbbox2_3D(leftOutput,margin=1)\n",
    "    rightBounds128 = torchbbox2_3D(rightOutput,margin=1)\n",
    "    leftCoarseBounds = torchRescaleBounds(leftBounds128,coarseCropped.shape,leftOutput.shape)\n",
    "    rightCoarseBounds = torchRescaleBounds(rightBounds128,coarseCropped.shape,rightOutput.shape)\n",
    "    leftBounds = fitInBounds(leftCoarseBounds,coarseBounds)\n",
    "    rightBounds = fitInBounds(rightCoarseBounds,coarseBounds)\n",
    "    leftCropped = torchCrop(originalImage,leftBounds)\n",
    "    rightCropped = torchCrop(originalImage,rightBounds)\n",
    "\n",
    "    print(LROutput.shape)\n",
    "    #LRCoarseSize = ndimage.zoom(LROutput, (coarseCropped.shape[0]/128,coarseCropped.shape[1]/128,coarseCropped.shape[2]/128), order=0)\n",
    "    LRCoarseSize = torch.nn.functional.interpolate(LROutput,size=coarseCropped.shape[2:],mode='nearest')\n",
    "    LRFullSizeMask[:,:,coarseBounds[0]:coarseBounds[1],coarseBounds[2]:coarseBounds[3],coarseBounds[4]:coarseBounds[5]] = LRCoarseSize\n",
    "    LRFullSizeMask = torch.where(LRFullSizeMask > 0,1,0)\n",
    "    LRMaskDilated = pytorchBinaryDilation(LRFullSizeMask)\n",
    "\n",
    "\n",
    "\n",
    "    # Get and post-process left lobe model output\n",
    "    leftInput = torchPrep(leftCropped)\n",
    "    leftLobeOutput = torchGetModelOutput(leftInput,leftModel)\n",
    "    leftLobeOutput = torch.nn.functional.interpolate(leftLobeOutput, size=leftCropped.shape[2:], mode='nearest')\n",
    "    # Get and post-process right lobe model output\n",
    "    rightInput = torchPrep(rightCropped)\n",
    "    rightLobeOutput = torchGetModelOutput(rightInput,rightModel)       \n",
    "    rightLobeOutput = torch.nn.functional.interpolate(rightLobeOutput, size=rightCropped.shape[2:], mode='nearest')\n",
    "\n",
    "\n",
    "    #adjust right lobe output (0,1,2,3) to (0,3,4,5)\n",
    "    rightLobeOutput = rightLobeOutput + 2\n",
    "    rightLobeOutput = torch.where(rightLobeOutput==2,0,rightLobeOutput)\n",
    "    \n",
    "    #Assemble final mask\n",
    "    finalMask = torch.zeros(originalImage.shape).cuda()\n",
    "    leftFullSize = torch.zeros(originalImage.shape).cuda()\n",
    "    rightFullSize = torch.zeros(originalImage.shape).cuda()\n",
    "    leftFullSize[:,:,leftBounds[0]:leftBounds[1],leftBounds[2]:leftBounds[3],leftBounds[4]:leftBounds[5]] = leftLobeOutput\n",
    "    rightFullSize[:,:,rightBounds[0]:rightBounds[1],rightBounds[2]:rightBounds[3],rightBounds[4]:rightBounds[5]] = rightLobeOutput\n",
    "    finalMask = torch.where(leftFullSize > 0, leftFullSize, finalMask)\n",
    "    finalMask = torch.where(rightFullSize > 0, rightFullSize, finalMask)\n",
    "\n",
    "    finalMask = torch.where(LRMaskDilated > 0,finalMask,0)\n",
    "    #finalMask = connectedComponents(finalMask)\n",
    "\n",
    "    finalMask = finalMask.to(torch.uint8)\n",
    "    return finalMask\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matFile = loadmat(\"/home/gabriela/Documents/gabiSegmentationProject/B22T50_AdamWRMSScreening0_Sep13_162154corrected.mat\")\n",
    "image = matFile['B22T50_AdamWRMSScreening0_Sep13_162154']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'B22T50_AdamWRMSScreening0_Sep13_162154'])\n"
     ]
    }
   ],
   "source": [
    "print(matFile.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2061\n"
     ]
    }
   ],
   "source": [
    "print(np.min(image),np.max(image))\n",
    "image = 1 + ((image - 1024) / 1000)\n",
    "image = np.clip(image, 0, 1) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.tensor(image).float().unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, segInput = fullPytorchPipeline(image,coarseModel,LRModel,leftModel,rightModel,device)\n",
    "viewer = napari.view_image(segInput.cpu().numpy())\n",
    "viewer.add_image(output.cpu().numpy(),contrast_limits=(0,2),colormap='gist_earth',opacity=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 188, 280, 89])\n",
      "lr\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 1, 157, 146, 81])\n",
      "torch.Size([1, 1, 156, 116, 82])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Image layer 'Image [1]' at 0x7fb81d72fad0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageSeg = fullPytorchPipeline(image,coarseModel,LRModel,leftModel,rightModel,device)\n",
    "viewer = napari.view_image(image.cpu().numpy())\n",
    "viewer.add_image(imageSeg.cpu().numpy(),contrast_limits=(0,5),colormap='gist_earth',opacity=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B10_00.mat', 'B14_00.mat', 'B18_00.mat', 'B20_00.mat', 'B21_00.mat', 'B22_00.mat', 'B23_00.mat', 'B24_00.mat', 'B25_00.mat', 'B26_00.mat', 'B27_00.mat', 'B28_00.mat', 'B29_00.mat', 'B30_00.mat', 'B31_00.mat', 'B32_00.mat', 'B37_00.mat', 'B38_00.mat', 'B39_00.mat', 'B40_00.mat', 'B41_00.mat', 'B43_00.mat', 'B44_00.mat', 'B45_00.mat', 'B9_00.mat']\n",
      "0 2649\n"
     ]
    }
   ],
   "source": [
    "print(dataset.imageList)\n",
    "inhale,exhale,filename = dataset[5]\n",
    "inhale = inhale.unsqueeze(0)\n",
    "exhale = exhale.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'Image [1]' at 0x7f1136cd5cd0>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.view_image(exhale.cpu().numpy())\n",
    "viewer.add_image(finalMask_exhale.cpu().numpy(),contrast_limits=(0,5),colormap='gist_earth',opacity=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52160\n",
      "866\n",
      "4\n",
      "3\n",
      "6\n",
      "68\n",
      "438\n",
      "1\n",
      "2\n",
      "1\n",
      "23\n",
      "1\n",
      "10\n",
      "14\n",
      "61\n",
      "1\n",
      "3\n",
      "2\n",
      "10\n",
      "9\n",
      "6\n",
      "33\n",
      "1\n",
      "11\n",
      "1\n",
      "{628067: 52160}\n",
      "torch.Size([1, 1, 188, 280, 89])\n",
      "lr\n",
      "2\n",
      "463052\n",
      "1\n",
      "2\n",
      "1\n",
      "128\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "96\n",
      "3\n",
      "2\n",
      "3\n",
      "33\n",
      "5\n",
      "1\n",
      "1\n",
      "53\n",
      "6\n",
      "19\n",
      "1\n",
      "1\n",
      "1\n",
      "{201311: 463052}\n",
      "262282\n",
      "1\n",
      "{201311: 262282}\n",
      "200747\n",
      "1\n",
      "1\n",
      "14\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "{273237: 200747}\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 1, 159, 146, 82])\n",
      "torch.Size([1, 1, 154, 116, 83])\n"
     ]
    }
   ],
   "source": [
    "finalMask_exhale = fullPytorchPipeline(exhale,coarseModel,LRModel,leftModel,rightModel,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4095\n",
      "0 4095\n",
      "[0 1]\n",
      "0\n",
      " 4095{661593: 17970, 729669: 15007}\n",
      "torch.Size([1, 1, 43, 60, 4])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "[0 1 2]\n",
      "0 4095\n",
      "{48425: 306843, 131083: 81818, 1457273: 31980}\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "index is out of bounds for dimension with size 0\n",
      "['B10_00.mat']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (inhale,exhale, filename) in enumerate(loader):\n",
    "       # inhale = inhale.squeeze(0).squeeze(0).numpy()\n",
    "        #print(np.min(inhale),np.max(inhale))\n",
    "        diagnostics = [False,False,False,False,False,True]\n",
    "        try:\n",
    "            finalMask_inhale = fullPytorchPipeline(inhale,coarseModel,LRModel,leftModel,rightModel,device)\n",
    "            #inhale_seg = nib.Nifti1Image(finalMask_inhale.astype(np.uint8), affine=np.eye(4))\n",
    "            #nib.save(inhale_seg, f'/home/gabriela/Documents/validation_cases/inhale_masks/{filename}_inhale.nii')\n",
    "            finalMask_exhale = fullPytorchPipeline(exhale,coarseModel,LRModel,leftModel,rightModel,device)\n",
    "            #exhale_seg = nib.Nifti1Image(finalMask_exhale.astype(np.uint8), affine=np.eye(4))\n",
    "            #nib.save(exhale_seg, f'/home/gabriela/Documents/validation_cases/exhale_masks/{filename}_exhale.nii')\n",
    "            Viewer = napari.view_image(finalMask_inhale.cpu().numpy())\n",
    "            if np.array(diagnostics).any():\n",
    "                break\n",
    "                \n",
    "        except IndexError as e:\n",
    "            print(e)\n",
    "            print(filename)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [0.99539514 0.9478496  0.94130024 0.93314625 0.88585846 0.94128153]\n",
      "std: [0.0013794  0.01497322 0.01926996 0.03128335 0.05807555 0.01709858]\n",
      "median: [0.99547702 0.95141895 0.94552664 0.94149879 0.90300134 0.94462566]\n",
      "minimum: [0.99123203 0.83449172 0.81887321 0.63325052 0.57119522 0.83333865]\n",
      "maximum: [0.9986088  0.96713803 0.96954426 0.96500875 0.94338542 0.96561018]\n",
      "percentile25: [0.99445298 0.94268888 0.93568856 0.92967128 0.87999438 0.93456552]\n",
      "percentile75: [0.99646507 0.95708696 0.95356125 0.94964599 0.91893935 0.95257833]\n",
      "Number of rows with at least one IoU below .8: 31\n",
      "Number of rows with at least one IoU below .1: 0\n"
     ]
    }
   ],
   "source": [
    "#calculate median, min, max, 25% 75% percentile of each iou\n",
    "median = np.median(allIoUs,axis=0)\n",
    "minimum = np.min(allIoUs,axis=0)\n",
    "maximum = np.max(allIoUs,axis=0)\n",
    "percentile25 = np.percentile(allIoUs,25,axis=0)\n",
    "percentile75 = np.percentile(allIoUs,75,axis=0)\n",
    "mean = np.mean(allIoUs,axis=0)\n",
    "std = np.std(allIoUs,axis=0)\n",
    "print(f\"mean: {mean}\")\n",
    "print(f\"std: {std}\")\n",
    "print(f\"median: {median}\")\n",
    "print(f\"minimum: {minimum}\")\n",
    "print(f\"maximum: {maximum}\")\n",
    "print(f\"percentile25: {percentile25}\")\n",
    "print(f\"percentile75: {percentile75}\")\n",
    "\n",
    "#count rows with IoUs below .8\n",
    "count = 0\n",
    "completeFailCount = 0\n",
    "for row in allIoUs:\n",
    "    flag = np.array([True for val in row if val < 0.8]).any()\n",
    "    completeFail = np.array([True for val in row if val < .10]).any()\n",
    "    if flag:\n",
    "        count += 1\n",
    "    if completeFail:\n",
    "        completeFailCount += 1\n",
    "print(f\"Number of rows with at least one IoU below .8: {count}\")\n",
    "print(f\"Number of rows with at least one IoU below .1: {completeFailCount}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index 0 is out of bounds for axis 0 with size 0\n",
    "['17704Z_EXP_image.nii.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['22169K_EXP_image.nii.gz']: [0.99569558 0.95644109 0.94816801 0.89872881 0.76741352 0.94431173]\n",
      "['15460N_EXP_image.nii.gz']: [0.99815988 0.93218118 0.90603531 0.93303655 0.73704107 0.89392002]\n",
      "['12596X_INSP_image.nii.gz']: [0.9935656  0.96171443 0.95769247 0.87585702 0.75122895 0.95124937]\n",
      "['11401D_EXP_image.nii.gz']: [0.9979965  0.92898779 0.9224925  0.83855884 0.75783769 0.91827999]\n",
      "['11015Y_EXP_image.nii.gz']: [0.9945723  0.96174481 0.95938263 0.90511807 0.75851981 0.96037623]\n",
      "['10795T_EXP_image.nii.gz']: [0.99700719 0.8665107  0.82409845 0.83751104 0.57155334 0.91124893]\n",
      "['16553Z_EXP_image.nii.gz']: [0.99534504 0.9406398  0.92054885 0.91516137 0.63344579 0.86685302]\n",
      "['12895H_EXP_image.nii.gz']: [0.99523863 0.91570598 0.93001188 0.93450595 0.74939736 0.94516784]\n",
      "['19370G_INSP_image.nii.gz']: [0.99365281 0.96236574 0.96522614 0.87827307 0.73182768 0.96152413]\n",
      "['18558T_INSP_image.nii.gz']: [0.99521442 0.95785104 0.95418646 0.63325052 0.57119522 0.95869274]\n",
      "['11659Q_INSP_image.nii.gz']: [0.99266049 0.96040603 0.95930397 0.73008352 0.61007009 0.96363539]\n",
      "['18370B_EXP_image.nii.gz']: [0.99612413 0.95255431 0.94197488 0.81042042 0.69789905 0.92970623]\n",
      "['11764N_INSP_image.nii.gz']: [0.99369364 0.95651953 0.96040541 0.83523841 0.79912458 0.96105793]\n",
      "['24704Q_INSP_image.nii.gz']: [0.99366459 0.9625622  0.95755972 0.90391967 0.7904707  0.95847835]\n",
      "['22210H_EXP_image.nii.gz']: [0.99494518 0.95102777 0.95616506 0.95423769 0.77788391 0.92214   ]\n",
      "['22455L_EXP_image.nii.gz']: [0.99722382 0.93814826 0.93824512 0.89118657 0.74713101 0.94080094]\n",
      "['19612E_EXP_image.nii.gz']: [0.99626145 0.94912594 0.94199182 0.90401768 0.78394452 0.94346086]\n",
      "['23700D_EXP_image.nii.gz']: [0.99611831 0.95148672 0.9327338  0.94623052 0.78559684 0.91964922]\n",
      "['22842S_EXP_image.nii.gz']: [0.9968234  0.95006328 0.94836054 0.82658092 0.72279216 0.93973663]\n",
      "['15987B_EXP_image.nii.gz']: [0.99528379 0.94504477 0.92692855 0.8177524  0.74684181 0.92428615]\n",
      "['14155D_INSP_image.nii.gz']: [0.99174246 0.96255005 0.95820072 0.88451473 0.76276089 0.96028816]\n",
      "['16469K_INSP_image.nii.gz']: [0.99531131 0.94858132 0.93965592 0.82811741 0.62341862 0.92359767]\n",
      "['13176G_EXP_image.nii.gz']: [0.99583602 0.906168   0.85758077 0.91822379 0.77780801 0.92333783]\n",
      "['12204G_INSP_image.nii.gz']: [0.99665961 0.96090023 0.96439126 0.91452086 0.74401704 0.93739514]\n",
      "['10182M_EXP_image.nii.gz']: [0.99455532 0.94341148 0.91843279 0.88533757 0.72032251 0.92689241]\n",
      "['13915S_INSP_image.nii.gz']: [0.9944438  0.95865568 0.94925466 0.8561116  0.76030091 0.94505297]\n",
      "['10849Q_EXP_image.nii.gz']: [0.99402233 0.95051433 0.93099909 0.85076113 0.58446795 0.8816572 ]\n",
      "['12008G_INSP_image.nii.gz']: [0.9962603  0.94201385 0.92987476 0.95695247 0.79263236 0.92691305]\n",
      "['22279R_INSP_image.nii.gz']: [0.99553671 0.96175373 0.95028121 0.94673929 0.78838643 0.95489253]\n",
      "['16469K_EXP_image.nii.gz']: [0.99627894 0.94395219 0.94534772 0.78506557 0.62261102 0.90588275]\n",
      "['12022A_EXP_image.nii.gz']: [0.99439214 0.96394556 0.95109573 0.94867526 0.70963364 0.90339805]\n"
     ]
    }
   ],
   "source": [
    "badFiles = []\n",
    "with open('../ious.txt','r') as iouf:\n",
    "    lines = iouf.readlines()\n",
    "    for line in lines:\n",
    "        ious, filename = line.split('-')\n",
    "        ious = ious.strip().strip('[').strip(']').split(',')\n",
    "        ious = np.array([float(val) for val in ious])\n",
    "        filename = filename.strip()\n",
    "        if (ious < .8).any():\n",
    "            #print(f\"{filename} has at least one IoU below .8\")\n",
    "            print(f\"{filename}: {ious}\")\n",
    "            badFiles.append(filename)\n",
    "badFiles = [val[2:-2] for val in badFiles]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (75,447,674,848,855) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/aaronluong/pft/twoTimepoint.txt',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([False,  True]), array([19, 12]))\n"
     ]
    }
   ],
   "source": [
    "phase = ['INSP' in val for val in badFiles]\n",
    "print(np.unique(phase,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalgold_baseline\n",
      "2.0                   9\n",
      "3.0                   9\n",
      "0.0                   7\n",
      "4.0                   3\n",
      "1.0                   2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "badPatientIds = [val[:6] for val in badFiles]\n",
    "badSubset = df[df['sid'].isin(badPatientIds)]\n",
    "print(badSubset.loc[:,['finalgold_baseline']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1490, 682, 319, 168, 120, 94, 794, 348, 1179, 1081, 191, 1052, 203, 1801, 1492, 1518, 1198, 1680, 1576, 732, 515, 791, 394, 271, 22, 491, 100, 237, 1503, 790, 240]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "indices = [dataset.findIndex(filename) for filename in badFiles]\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`selem` is a deprecated argument name for `binary_dilation`. It will be removed in version 1.0. Please use `footprint` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9982706631260615, 0.9340230302255383, 0.9073126849027756, 0.9362145931751, 0.7379043964674135, 0.8966783809776152] - 15460N_EXP_image.nii.gz\n",
      "[0. 1. 2. 3. 4. 5.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Image layer 'diff' at 0x7f316bae2940>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "idx = indices[1]\n",
    "originalImage,originalLabel, filename = dataset[idx]\n",
    "originalLabel = originalLabel.to(device)\n",
    "originalImage = originalImage.unsqueeze(0)\n",
    "finalMask = fullPipeline(originalImage,coarseModel,LRModel,leftModel,rightModel,device,diagnostics=[False,False,False,True,True,False])\n",
    "originalLabel = originalLabel.squeeze(0).squeeze(0).cpu().numpy()\n",
    "IoUs = compute_iou(finalMask,originalLabel,6)\n",
    "print(f\"{IoUs} - {filename}\")\n",
    "diff = abs(finalMask - originalLabel)\n",
    "print(np.unique(diff))\n",
    "viewer.add_image(originalImage.squeeze(0).squeeze(0).cpu().numpy())\n",
    "viewer.add_image(originalLabel,colormap=\"gist_earth\",contrast_limits=(0,5),opacity=.5)\n",
    "viewer.add_image(finalMask,colormap=\"gist_earth\",contrast_limits=(0,5),opacity=.5)\n",
    "viewer.add_image(diff,colormap=\"red\",opacity=.5,contrast_limits=(0,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imageDirectory):\n",
    "        self.imageList = sorted([file for file in os.listdir(imageDirectory) if \"image\" in file])\n",
    "        #self.imageList = sorted([file for file in os.listdir(imageDirectory)])\n",
    "        self.labelList = sorted([file for file in os.listdir(imageDirectory) if \"mask\" in file])\n",
    "        self.imageDirectory = imageDirectory\n",
    "    \n",
    "    def __len__(self):\n",
    "        x = len(self.imageList)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imagePath = os.path.join(self.imageDirectory, self.imageList[idx])\n",
    "        labelPath = os.path.join(self.imageDirectory, self.labelList[idx])\n",
    "        image = nib.load(imagePath).get_fdata()\n",
    "        image = 1+ image/1000\n",
    "        image = np.clip(image,0,1) * 255\n",
    "        label = nib.load(labelPath).get_fdata()\n",
    "        #matfile = loadmat(imagePath)\n",
    "        #inhale = np.array(matfile['T00'], dtype=np.int32)\n",
    "        #exhale = np.array(matfile['T50'], dtype=np.int32)\n",
    "        #print(np.min(inhale),np.max(inhale))\n",
    "        #inhale = 1 + ((inhale - 1024) / 1000)\n",
    "        #inhale = np.clip(inhale, 0, 1) * 255\n",
    "        #label = nib.load(labelPath).get_fdata()\n",
    "        #inhale = torch.from_numpy(inhale).float().unsqueeze(0)\n",
    "        #exhale = 1 + ((exhale-1024) / 1000)\n",
    "        #exhale = np.clip(exhale, 0, 1) * 255\n",
    "        #label = nib.load(labelPath).get_fdata()\n",
    "        #exhale = torch.from_numpy(exhale).float().unsqueeze(0)\n",
    "        #label = torch.from_numpy(label).float().unsqueeze(0)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def findIndex(self,filename):\n",
    "        for i in range(len(self.imageList)):\n",
    "            if filename == self.imageList[i]:\n",
    "                return i\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 534)\n"
     ]
    }
   ],
   "source": [
    "testDataset = testdataset(\"/home/gabriela/Documents/newCOPD_raw\")\n",
    "image, label = testDataset[5]\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 533\n",
      "(512, 512, 534)\n",
      "(87, 380, 35, 482, 20, 534)\n",
      "25 533\n",
      "(87, 380, 35, 482, 20, 534)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def bbox2_3D(img,margin=5):\n",
    "    r = np.any(img, axis=(1, 2))\n",
    "    c = np.any(img, axis=(0, 2))\n",
    "    z = np.any(img, axis=(0, 1))\n",
    "    xmin, xmax = np.where(r)[0][[0, -1]]\n",
    "    ymin, ymax = np.where(c)[0][[0, -1]]\n",
    "    zmin, zmax = np.where(z)[0][[0, -1]]\n",
    "    print(zmin,zmax)\n",
    "    xmin = max(0,xmin-margin)\n",
    "    xmax = min(img.shape[0],xmax+margin)\n",
    "    ymin = max(0,ymin-margin)\n",
    "    ymax = min(img.shape[1],ymax+margin)\n",
    "    zmin = max(0,zmin-margin)\n",
    "    zmax = min(img.shape[2],zmax+margin)\n",
    "    return xmin, xmax, ymin, ymax, zmin, zmax\n",
    "\n",
    "test = label\n",
    "assert (test == torch.tensor(test).numpy()).all()\n",
    "\n",
    "\n",
    "\n",
    "viewer = napari.view_image(test)\n",
    "viewer.add_image(torch.tensor(test).numpy())\n",
    "torchDebugBox = np.zeros_like(test)\n",
    "xmin,xmax,ymin,ymax,zmin,zmax = torchbbox2_3D(torch.tensor(test))\n",
    "torchDebugBox[xmin:xmax,ymin:ymax,zmin:zmax] = 1\n",
    "xmin,xmax,ymin,ymax,zmin,zmax = bbox2_3D(test)\n",
    "npDebugBox = np.zeros_like(test)\n",
    "npDebugBox[xmin:xmax,ymin:ymax,zmin:zmax] = 1\n",
    "viewer.add_image(npDebugBox)\n",
    "viewer.add_image(torchDebugBox)\n",
    "print(test.shape)\n",
    "print(torchbbox2_3D(torch.tensor(test)))\n",
    "print(bbox2_3D(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'final' at 0x7f0446bb4250>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallImage = ndimage.zoom(image,(128/image.shape[0],128/image.shape[1],128/image.shape[2]),order=0)\n",
    "thresholdedImage = smallImage > 128\n",
    "#eroded = ndimage.binary_erosion(dustedImage,structure=morphology.ball(1))\n",
    "flipped = np.logical_not(thresholdedImage)\n",
    "background = connectedComponents(flipped)\n",
    "final = np.logical_not(background)\n",
    "final = ndimage.binary_erosion(final,structure=morphology.ball(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorchGetLargest(tensor, num = None, ignoreBackground=True,override_comp_id=None):\n",
    "    connected_components = connected_components_labeling(tensor)\n",
    "    if override_comp_id is None:\n",
    "        if num is None:\n",
    "            num = len(tensor.unique()-1)\n",
    "        tensorView = tensor.cpu().numpy()\n",
    "        print(np.unique(tensorView))\n",
    "        viewer = napari.view_image(tensorView)\n",
    "        viewer.add_image(connected_components.cpu().numpy())\n",
    "        volume_count = {}\n",
    "        for component in connected_components.unique():\n",
    "            if component == 0 and ignoreBackground:  # background\n",
    "                continue\n",
    "            volume_count[component.item()] = torch.sum(connected_components == component).item()\n",
    "\n",
    "        print(volume_count)\n",
    "\n",
    "        # Sort by volume\n",
    "        sorted_components = sorted(volume_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Take two largest components\n",
    "        largest_components = [comp_id for comp_id, _ in sorted_components[:num]]\n",
    "    else:\n",
    "        largest_components = override_comp_id\n",
    "\n",
    "    # Create a new 3D array with only the two largest components\n",
    "    new_array_3d = torch.zeros_like(tensor)\n",
    "    for comp_id in largest_components:\n",
    "        new_array_3d[connected_components == comp_id] = tensor[connected_components == comp_id].unique()[0]   # or comp_id to retain original labels\n",
    "\n",
    "    return new_array_3d\n",
    "    \n",
    "\n",
    "def pytorchBinaryErosion(tensor):\n",
    "    ball = morphology.ball(2)\n",
    "    struct_elem = torch.tensor(ball, dtype=torch.float32)\n",
    "    struct_elem = struct_elem.view(1, 1, *struct_elem.size()).cuda()\n",
    "\n",
    "    # Perform 3D convolution with structuring element\n",
    "    conv_result = torch.nn.functional.conv3d(tensor.float(), struct_elem, padding=2)\n",
    "\n",
    "    # Binary erosion is equivalent to finding where the convolution result\n",
    "    # is equal to the sum of the structuring element\n",
    "    erosion_result = (conv_result == struct_elem.sum().item()).float()\n",
    "    return erosion_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90671/2607004415.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flippedTensor = torch.tensor(torch.logical_not(thresholdedTensor),dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Image layer 'Image [1]' at 0x7f00c6f33450>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorImage = torch.tensor(image).unsqueeze(0).unsqueeze(0).cuda()\n",
    "smallTensor = torch.nn.functional.interpolate(tensorImage,size=(128,128,128),mode='nearest')\n",
    "thresholdedTensor = (smallTensor > 128).squeeze(0).squeeze(0)\n",
    "flippedTensor = torch.tensor(torch.logical_not(thresholdedTensor),dtype=torch.uint8)\n",
    "ccTensor = pytorchGetLargest(flippedTensor,ignoreBackground=False,override_comp_id=[flippedTensor[0,0,0].item()])\n",
    "finalTensor = pytorchBinaryErosion(torch.logical_not(ccTensor).unsqueeze(0).unsqueeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93204/3170253203.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flippedTensor = torch.tensor(torch.logical_not(thresholdedTensor),dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy: 0.19847464561462402\n",
      "Torch: 0.023997068405151367\n",
      "322887.0\n",
      "Numpy: 0.1783769130706787\n",
      "Torch: 0.02505660057067871\n",
      "712212.0\n",
      "Numpy: 0.18296170234680176\n",
      "Torch: 0.024211406707763672\n",
      "16958.0\n",
      "Numpy: 0.1741933822631836\n",
      "Torch: 0.027007579803466797\n",
      "17880.0\n",
      "Numpy: 0.1802518367767334\n",
      "Torch: 0.02519965171813965\n",
      "18358.0\n",
      "Numpy: 0.17082643508911133\n",
      "Torch: 0.02610468864440918\n",
      "16940.0\n",
      "Numpy: 0.1807100772857666\n",
      "Torch: 0.022684097290039062\n",
      "18131.0\n",
      "Numpy: 0.17917966842651367\n",
      "Torch: 0.02402329444885254\n",
      "18567.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     image \u001b[39m=\u001b[39m testDataset[i]\n\u001b[1;32m      4\u001b[0m     tensorImage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(image)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      5\u001b[0m     numpyStart \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m, in \u001b[0;36mtestdataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m image \u001b[39m=\u001b[39m nib\u001b[39m.\u001b[39mload(imagePath)\u001b[39m.\u001b[39mget_fdata()\n\u001b[1;32m     16\u001b[0m image \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m+\u001b[39m image\u001b[39m/\u001b[39m\u001b[39m1000\u001b[39m\n\u001b[0;32m---> 17\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(image,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39m#matfile = loadmat(imagePath)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m#inhale = np.array(matfile['T00'], dtype=np.int32)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m#exhale = np.array(matfile['T50'], dtype=np.int32)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m#exhale = torch.from_numpy(exhale).float().unsqueeze(0)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m#label = torch.from_numpy(label).float().unsqueeze(0)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(10):\n",
    "    image = testDataset[i]\n",
    "    tensorImage = torch.tensor(image).unsqueeze(0).unsqueeze(0).cuda()\n",
    "    numpyStart = time.time()\n",
    "    smallImage = ndimage.zoom(image,(128/image.shape[0],128/image.shape[1],128/image.shape[2]),order=0)\n",
    "    thresholdedImage = smallImage > 128\n",
    "    #eroded = ndimage.binary_erosion(dustedImage,structure=morphology.ball(1))\n",
    "\n",
    "    flipped = np.logical_not(thresholdedImage)\n",
    "    background = connectedComponents(flipped)\n",
    "    final = np.logical_not(background)\n",
    "    final = ndimage.binary_erosion(final,structure=morphology.ball(2))\n",
    "    numpyEnd = time.time()\n",
    "    torchStart = time.time()\n",
    "    smallTensor = torch.nn.functional.interpolate(tensorImage,size=(128,128,128),mode='nearest')\n",
    "    thresholdedTensor = (smallTensor > 128).squeeze(0).squeeze(0)\n",
    "    flippedTensor = torch.tensor(torch.logical_not(thresholdedTensor),dtype=torch.uint8)\n",
    "    ccTensor = pytorchGetLargest(flippedTensor,ignoreBackground=False,override_comp_id=[flippedTensor[0,0,0].item()])\n",
    "    #viewer.add_image(ccTensor.cpu().numpy())\n",
    "    finalTensor = pytorchBinaryErosion(torch.logical_not(ccTensor).unsqueeze(0).unsqueeze(0))\n",
    "    torchEnd = time.time()\n",
    "    print(f'Numpy: {numpyEnd-numpyStart}')\n",
    "    print(f'Torch: {torchEnd-torchStart}')\n",
    "    print(np.sum(abs(finalTensor.squeeze(0).squeeze(0).cpu().numpy() - final),axis=(0,1,2)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
